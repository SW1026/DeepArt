{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의 및 학습\n",
    "\n",
    "이번 챕터에서는 우리가 준비한 데이터셋에 실제로 사용할 RNN 모델을 정의하고 학습해보도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 인풋/아웃풋 구조 준비\n",
    "\n",
    "우리가 사용할 Char-RNN구조는 일정 길이의 시퀀스 단위로 인풋을 처리합니다. \n",
    "\n",
    "즉, 일정 길이의 인풋 시퀀스를 받아서 시퀀스 내의 각 스텝이 다음 스텝의 인풋과 같은 엘리먼트를 아웃풋으로 출력하도록 학습되게 하는 것입니다.\n",
    "\n",
    "Char-RNN은 한 스텝의 인풋 씩 받아서 다음 스텝을 생성하는 모델이지만, 학습할 때는 일정 길이의 시퀀스 단위로 학습을 하도록 합니다. \n",
    "\n",
    "\n",
    "* 사실 원래 개념상으로는 데이터 전체에 대해서 RNN이 backpropagation하면서 학습이 이루어져야 합니다.하지만 메모리의 문제 때문에 그렇게 할 수 없으므로 일정 길이의 묶음으로 나누어서 학습을 하기 위해서 이런 방식을 사용합니다. (물론, Sequence-to-sequence 모델에서는 우리의 목적에 따른 길이의 데이터 시퀀스로 학습을 하겠죠.)\n",
    "\n",
    "\n",
    "따라서, 학습할 때에는 적당한 길이로 잘라서 학습하고, 나중에 생성할 때에는 한번에 한 엘리먼트씩 생성됩니다. (생성된 아웃풋을 다음 스텝의 인풋으로 사용.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(즉, 제한된 sequence길이 내에서 unrolling을 하면서 학습이 이루어지도록 모델을 설계합니다. 대신, 학습 완료 후 한번에 하나씩 생성시에는 바로 전 스텝으로부터 hidden state만 받아오면 되기 때문에 길이에 제한없이 생성이 가능합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import time, os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행해봅시다.\n",
    "\n",
    "먼저, 저장해놓았던 데이터를 불러옵시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dir = \"./preprocessed_data/\"\n",
    "\n",
    "with open(preprocessed_dir + \"vocab_size.p\", \"rb\") as fp:   \n",
    "    vocab_size = pickle.load(fp)\n",
    "    \n",
    "with open(preprocessed_dir + \"input_sequences.p\", \"rb\") as fp:   \n",
    "    input_sequences = pickle.load(fp)\n",
    "    \n",
    "with open(preprocessed_dir + \"label_sequences.p\", \"rb\") as fp:   \n",
    "    label_sequences = pickle.load(fp)\n",
    "\n",
    "with open(preprocessed_dir + \"mel_set.p\", \"rb\") as fp:   \n",
    "    mel_set = pickle.load(fp)\n",
    "\n",
    "with open(preprocessed_dir + \"mel_i_v.p\", \"rb\") as fp:   \n",
    "    mel_i_v = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_RNN(object):\n",
    "    def __init__(self, \n",
    "                 sess, \n",
    "                 batch_size=16, \n",
    "                 learning_rate=0.001,\n",
    "                 num_layers = 3,\n",
    "                 num_vocab = 1,\n",
    "                 hidden_layer_units = 64,\n",
    "                 sequence_length = 8,\n",
    "                 data_dir='preprocessed_data/',\n",
    "                 checkpoint_dir='checkpoint/',\n",
    "                 sample_dir=None):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_layer_units = hidden_layer_units\n",
    "        self.num_vocab = num_vocab\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data_dir = data_dir\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        # input place holders\n",
    "        self.X = tf.placeholder(dtype=tf.int32, shape=[None, self.sequence_length], name='input')\n",
    "        self.Y = tf.placeholder(dtype=tf.int32, shape=[None, self.sequence_length], name='label')\n",
    "        \n",
    "        self.x_one_hot = tf.one_hot(self.X, self.num_vocab)\n",
    "        self.y_one_hot = tf.one_hot(self.Y, self.num_vocab)\n",
    "\n",
    "        self.optimizer, self.sequence_loss, self.curr_state = self.build_model()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\", self.sess.graph)\n",
    "\n",
    "\n",
    "    def create_rnn_cell(self):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units = self.hidden_layer_units,\n",
    "                                            state_is_tuple = True)\n",
    "        return cell\n",
    "\n",
    "\n",
    "    def create_rnn(self):\n",
    "        \n",
    "        multi_cells = tf.contrib.rnn.MultiRNNCell([self.create_rnn_cell()\n",
    "                                                   for _ in range(self.num_layers)],\n",
    "                                                   state_is_tuple=True)\n",
    "        self.multi_cells = rnn.DropoutWrapper(multi_cells, input_keep_prob=0.9, output_keep_prob=0.9)\n",
    "\n",
    "        # prepare initial state value\n",
    "        self.rnn_initial_state = self.multi_cells.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "        rnn_outputs, out_states = tf.nn.dynamic_rnn(multi_cells, self.x_one_hot, dtype=tf.float32, initial_state=self.rnn_initial_state)\n",
    "        return rnn_outputs, out_states\n",
    "\n",
    "\n",
    "    def build_model(self): \n",
    "        \n",
    "        rnn_output, self.out_state = self.create_rnn()\n",
    "        rnn_output_flat = tf.reshape(rnn_output, [-1, self.hidden_layer_units]) # [N x sequence_length, hidden]\n",
    "        \n",
    "        self.logits = tf.contrib.layers.fully_connected(rnn_output_flat, self.num_vocab, None)\n",
    "\n",
    "        # for generation \n",
    "        y_softmax = tf.nn.softmax(self.logits)         # [N x seqlen, vocab_size]\n",
    "        pred = tf.argmax(y_softmax, axis=1)       # [N x seqlen]\n",
    "        self.pred = tf.reshape(pred, [self.batch_size, -1]) # [N, seqlen]\n",
    "\n",
    "        y_flat = tf.reshape(self.y_one_hot, [-1, self.num_vocab]) # [N x sequence_length, vocab_size]\n",
    "\n",
    "        losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_flat, logits=self.logits)\n",
    "        sequence_loss = tf.reduce_mean(losses)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(sequence_loss)\n",
    "\n",
    "        tf.summary.scalar('training loss', sequence_loss)\n",
    "        self.merged_summary = tf.summary.merge_all()\n",
    "        \n",
    "        return opt, sequence_loss, self.out_state\n",
    "\n",
    "\n",
    "    ## save current model\n",
    "    def save_model(self, checkpoint_dir, step): \n",
    "        model_name = \"melodyRNN.model\"\n",
    "        model_dir = \"model\"\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)\n",
    "\n",
    "    ## load saved model\n",
    "    def load(self, checkpoint_dir):   \n",
    "        print(\" [*] Reading checkpoint...\")\n",
    "\n",
    "        model_dir = \"model\"\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            # self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            print(ckpt_name)\n",
    "            print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            self.saver.restore(self.sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def train(self, input_sequences, label_sequences, num_epochs): \n",
    "\n",
    "        ## initialize                         \n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sess.run(init_op)\n",
    "\n",
    "        if self.load(self.checkpoint_dir):\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            print(\" [!] Load failed...\")\n",
    "        \n",
    "        counter = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        ## loading model \n",
    "        if self.load(self.checkpoint_dir):\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        num_all_sequences = input_sequences.shape[0]\n",
    "        num_batches = int(num_all_sequences / self.batch_size)\n",
    "\n",
    "        loss_per_epoch = []\n",
    "        ## training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_time = time.time()\n",
    "                losses_per_epoch = []\n",
    "            \n",
    "                _, loss, logits, curr_state, summary_str = self.sess.run([self.optimizer, \n",
    "                                                             self.sequence_loss, \n",
    "                                                             self.logits, \n",
    "                                                             self.curr_state,\n",
    "                                                             self.merged_summary], \n",
    "                              feed_dict={\n",
    "                                self.X: input_sequences[batch_idx * self.batch_size:(batch_idx+1)*self.batch_size], \n",
    "                                self.Y: label_sequences[batch_idx * self.batch_size:(batch_idx+1)*self.batch_size] \n",
    "                                })\n",
    "\n",
    "                self.writer.add_summary(summary_str, epoch)\n",
    "\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" \\\n",
    "                    % (epoch, batch_idx, num_batches,\n",
    "                        time.time() - start_time,\n",
    "                        loss))\n",
    "                losses_per_epoch.append(loss)\n",
    "            loss_per_epoch.append(np.mean(np.array(losses_per_epoch)))\n",
    "            \n",
    "            counter += 1\n",
    "\n",
    "            # if np.mod(counter, 10) == 1:\n",
    "            self.save_model(self.checkpoint_dir, counter)\n",
    "\n",
    "                # # Get sample \n",
    "                # if np.mod(counter, 200) == 1:\n",
    "                #   self.get_sample(epoch, idx, 'train')\n",
    "                #   self.get_sample( epoch, idx, 'val')\n",
    "\n",
    "                # # Saving current model\n",
    "                # if np.mod(counter, 500) == 2:\n",
    "                #   self.save(args.checkpoint_dir, counter)\n",
    "            np.savetxt('avg_loss_txt/averaged_loss_per_epoch_' + str(epoch) + '.txt', loss_per_epoch) \n",
    "\n",
    "\n",
    "    ## generate melody from input\n",
    "    def predict(self, user_input_sequence, mel_i_v):\n",
    "        self.predict_opt = True\n",
    "        print(\"User input : \", user_input_sequence.shape)\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sess.run(init_op)\n",
    "\n",
    "        if self.load(self.checkpoint_dir):\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            print(\" [!] Load failed...\")\n",
    "            return\n",
    "\n",
    "        ## prepare input sequence\n",
    "        print('[1] preparing user input data') # done at prdeict.py\n",
    "\n",
    "        ## generate corresponding melody\n",
    "        print('[2] generating sequence from RNN')\n",
    "        print('firstly, iterating through input')\n",
    "        \n",
    "        hidden_state = self.sess.run(self.multi_cells.zero_state(self.batch_size, tf.float32))\n",
    "        \n",
    "        for i in range(user_input_sequence.shape[0]):\n",
    "#             print(i)\n",
    "#             print(user_input_sequence[i])\n",
    "            new_logits, prediction, hidden_state = self.sess.run([self.logits, self.pred, self.out_state], \n",
    "                                                feed_dict={self.X: user_input_sequence[i], self.rnn_initial_state: hidden_state})\n",
    "#             print(new_logits)\n",
    "#             print(prediction)\n",
    "        print(new_logits.shape)\n",
    "\n",
    "        print('secondly, generating')\n",
    "        generated_input_seq = []\n",
    "\n",
    "        for one_hot in new_logits:\n",
    "            generated_input_seq.append(np.argmax(one_hot))\n",
    "        generated_input_seq = np.expand_dims(np.array(generated_input_seq), axis=0)\n",
    "\n",
    "        ## generate melody \n",
    "        generated_melody = []\n",
    "        generated_melody_length = 0\n",
    "\n",
    "        while(generated_melody_length < 4):\n",
    "\n",
    "            generated_pred = self.sess.run([self.pred], \n",
    "                                            feed_dict={self.X: generated_input_seq})\n",
    "            for p in generated_pred[0][0]:\n",
    "                curr_curve = mel_i_v[p]\n",
    "                generated_melody_length += curr_curve[1]\n",
    "                if generated_melody_length > 4:\n",
    "                    break\n",
    "                else:\n",
    "                    generated_melody.append(curr_curve)\n",
    "                    generated_input_seq = generated_pred[-1]\n",
    "#         print(np.array(generated_melody).shape)\n",
    "        \n",
    "        return generated_melody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = model_RNN(sess, \n",
    "                     batch_size=16, \n",
    "                     learning_rate=0.001,\n",
    "                     num_layers = 3,\n",
    "                     num_vocab = vocab_size,\n",
    "                     hidden_layer_units = 64,\n",
    "                     sequence_length = 8,\n",
    "                     data_dir='preprocessed_data/')\n",
    "\n",
    "model.train(input_sequences, label_sequences, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 가지고 불특정 인풋에 대해 이어지는 멜로디를 생성하는 작업을 해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 노트북의 cpu 컴퓨팅 파워로는 학습을 제대로 진행하기가 어렵습니다. \n",
    "\n",
    "미리 동일한 코드로 2000 epoch 학습을 시켜서 저장해놓은 weight값을 불러와서 실제로 어떤식으로 결과물을 출력하는지 확인해보겠습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
