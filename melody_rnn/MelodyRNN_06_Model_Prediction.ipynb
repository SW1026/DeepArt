{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 모델을 활용한 멜로디 prediction\n",
    "\n",
    "자, 이제 드디어 이전 장에서 학습한 멜로디 모델을 사용해서 주어진 인풋에 대해 예측값을 출력해볼 차례입니다.\n",
    "\n",
    "이 모델을 사용해서 우리는 구글 마젠타 프로젝트에서 선보였던 AI_DUET같은 어플리케이션을 시도해볼 수 있습니다.\n",
    "\n",
    "[https://experiments.withgoogle.com/ai/ai-duet](https://experiments.withgoogle.com/ai/ai-duet)\n",
    "\n",
    "즉, 유저에게 인풋을 받은 후 이에 이어지는 멜로디를(학습된 데이터에 기반해서) AI가 생성해서 피드백하는 형태의 어플리케이션입니다.\n",
    "\n",
    "언제나처럼, 필요한 라이브러리를 로드해봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지면을 중복해서 사용하지 않기 위해 지난 챕터들에서 정의했던 데이터 관련 함수들과 모델 구조 정의를 간단한 파이썬 파일로 따로 만들어 두었습니다.(mel_op.py / model.py)\n",
    "\n",
    "이 파일을 로드하면 정의된 모든 함수를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mel_op import *\n",
    "from model import model_RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재는 유저의 미디 인풋을 받는 모듈 작업이 되어 있지 않기 때문에, 간단히 우리의 인풋 데이터 중 하나를 골라서 인풋으로 넣어보겠습니다.\n",
    "\n",
    "이를 위해 저장했던 데이터를 다시 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dir = \"./preprocessed_data/\"\n",
    "\n",
    "with open(preprocessed_dir + \"input_sequences.p\", \"rb\") as fp:   \n",
    "    input_sequences = pickle.load(fp)\n",
    "\n",
    "with open(preprocessed_dir + \"vocab_size.p\", \"rb\") as fp:   \n",
    "    vocab_size = pickle.load(fp)\n",
    "\n",
    "with open(preprocessed_dir + \"mel_set.p\", \"rb\") as fp:   \n",
    "    mel_set = pickle.load(fp)\n",
    "\n",
    "with open(preprocessed_dir + \"mel_i_v.p\", \"rb\") as fp:   \n",
    "    mel_i_v = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare as an batch\n",
    "def get_input_batch_sequence(input_seq, sequence_length):\n",
    "    \n",
    "    input_sequence_batches = []\n",
    "\n",
    "    num_seqs_per_song = max(int((len(input_seq) / sequence_length)) - 1, 0)\n",
    "\n",
    "    for ns in range(num_seqs_per_song):\n",
    "        batch = np.expand_dims(input_seq[ns * sequence_length:(ns+1) * sequence_length], axis=0)\n",
    "        input_sequence_batches.append(batch)\n",
    "\n",
    "    return np.array(input_sequence_batches)\n",
    "\n",
    "def preprocess_user_input(mel_arr):\n",
    "    curve_seq_list = []\n",
    "    curve_seq_list.append(create_curve_seq(mel_arr))\n",
    "\n",
    "    return curve_seq_list\n",
    "\n",
    "def predict_output(curve_arr, mel_i_v, sequence_length = 8):\n",
    "\n",
    "    ## prepare user input sequence with existing vocab in melody set\n",
    "    user_input_sequence = []\n",
    "    for curve in curve_arr:\n",
    "        similar_curve = find_similar_curve(curve, mel_set)\n",
    "        user_input_sequence.append(similar_curve)\n",
    "\n",
    "    print(user_input_sequence)\n",
    "    \n",
    "    ## pad zeros to the user input sequence\n",
    "    if len(user_input_sequence) < sequence_length:\n",
    "        user_input_sequence += [0] * (sequence_length - len(user_input_sequence))\n",
    "\n",
    "    input_sequence_as_batches = get_input_batch_sequence(user_input_sequence, sequence_length)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        model = model_RNN(sess, \n",
    "                         batch_size=1, \n",
    "                         learning_rate=0.001,\n",
    "                         num_layers = 3,\n",
    "                         num_vocab = vocab_size,\n",
    "                         hidden_layer_units = 64,\n",
    "                         sequence_length = 8,\n",
    "                         data_dir='generation_model/preprocessed_data/')\n",
    "\n",
    "    output_sequence = model.predict(np.array(input_sequence_as_batches), mel_i_v)\n",
    "\n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 8\n",
    "user_input_file = '~/Google Drive/data/nottingham-dataset/MIDI/melody/ashover1.mid'\n",
    "midi_obj = music21.converter.parse(user_input_file)\n",
    "mel_data = create_mel_data_each_file(midi_obj)\n",
    "mel_arr = []\n",
    "for key in sorted(mel_data.keys()):\n",
    "    mel_arr.append(mel_data[key])\n",
    "\n",
    "curve_arr = create_curve_seq(mel_arr)\n",
    "output_sequence = predict_output(curve_arr, mel_i_v, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-2, 1.0), (-2, 1.0), (-2, 1.0), (-2, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### AI-MELODY-DUET 예시\n",
    "\n",
    "아티스트로써 여러분의 창의력을 또한번 발휘할 수 있는 부분이라고 생각합니다. 일정 길이의 유저 인풋에 대해서 대응하는 멜로디를 AI가 연주하는 컨셉을 어떤 식으로 표현하는가에 따라 작품이 이루어질 수 있을 것입니다.\n",
    "\n",
    "여기서는 하나의 예시로 좌우로 나뉜 화면에서 유저의 입력 멜로디와 AI가 생성한 멜로디를 번갈아 보여주는 시각화 페이지를 보겠습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
